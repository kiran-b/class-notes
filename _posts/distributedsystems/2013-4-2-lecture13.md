---
layout: post
title: Relaxed Consistency Case Studies
categories: distributed-systems
---

# Lecture 13: PNUTS

<hr />
Brian F. Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, Adam
Silberstein, Philip Bohannon, Hans-Arno Jacobsen, Nick Puz, Daniel
Weaver and Ramana Yerneni.

PNUTS: Yahoo!'s Hosted Data Serving Platform. 

Proceedings of VLDB, 2008.
<hr />

## Introduction

- what is PNUTS' overall goal?
  - (world, browsers, data centers)
  - overall story similar to that of MegaStore
  - 10s of data centers ("sites") all over the world
  - web applications, e.g. mail, shopping, social net
    - each app probably runs at all sites
  - PNUTS keeps state for apps
    - per-user: profile, shopping cart, friend list
    - per-item: book popularity, user comments
  - app might need any piece of data at any data center
  - need to handle lots of concurrent updates to different data
    - e.g. lots of users must be able to add items to shopping cart at same time
    - thus 1000s of PNUTS servers
  - must cope well with partial failures
    - 1000s of servers => crashes must be frequent

### non-traditional architecture
- traditional would be central site, send all user requests there
- people have not been building this kind of system for very long
- but this is the way a lot of big web apps are going
  - similar recent designs: Amazon Dynamo, Facebook's Cassandra, MegaStore
- warning: not much consensus or traditional wisdom here

<hr />

## overview
- (diagram: 3 sites, browsers, web apps, tablet ctlrs, routers, storage units, MBs)
- each site has all data
- each table partitioned by key over storage units
  - tablet servers + routers know the partition plan

- why does it make sense for every site to have all data?
  - reads will be fast
  - read success only depends on local cluster, not WAN reliability

- what are some down-sides of a copy at each site?
  - (disk space prob not an issue for their uses)
  - updates will be slow, need to contact every site
  - need to keep replicas identical
    - all must see same updates, in same order

- what is the data and query model?
  - table
    - record
      - primary key + attributes (columns)
  - queries only by primary key, which must be unique
  - also supports range scan on primary key
  - reads/writes can probably be by column
    - so a write might replace just one column, not whole record

<hr>

## Updating

- app server gets web request, needs to write data in PNUTS
- need to update every site!
- why not just have app logic send update to every site?
  - what if app crashes after updating only some sites?
  - what if concurrent updates to same record?

- PNUTS has a "record master" for each record
  - all updates must go through that site
  - each record has a hidden column indicating site of record master
  - responsible storage unit executes updates one at a time per record
  - tells MB to broadcast update to all sites

### complete update story <small>(some guesswork)</small>

app wants to update some columns of a record, knows key

1. app sends key and update to local SU1
2. SU1 looks up record master for key: SI2
3. SU1 sends update request to router at SI2
4. router at SI2 forwards update to local SU2 for key
6. SU2 sends update to local Message Broker (MB)
7. MB stores on disk + backup MB, sends vers # to original app
  - how does MB know the vers #? maybe SU2 told it
  - or perhaps SU2 (not MB) replies to original app
8. MB sends update to router at every site
9. every site updates local copy

- puzzles:
  - paper says MB is commit point
  - does SU2 perform update before or after talking to MB?
    - what if SU2 crashes at a bad time?
    - maybe they are serious that they never recover a crashed SU?
  - who assigns version #, and when?
  - who replies to the app?

- writes seem like they'd be slow -- why does it make sense?
  - master likely to be local (if Alice is in SF, most of her writes are going to be in/around SF)
  - MB distribution is async (app does not wait)
  - down side: readers at non-master sites may see stale data

<hr />

## Reading

- multiple kinds of reads (section 2.2)
  - how does each work?
  - why is each needed?

1. read-any(k)
  - read from local SU
  - might return stale data (even if you just wrote!)
  - why: fast!
2. read-critical(k, required_version)
  - maybe read from local SU if it has vers >= required_version
  - otherwise read from master SU?
  - why: reflects app's writes; maybe fast
3. read-latest(k)
  - always read from master SU 
    - (paper says to go to record master "if local copy too stale")
    - no way of knowing "if too stale"
  - slow if master is remote!
  - why: app needs fresh data

- what if you need to increment a counter stored in a record?
  - app reads old value, increments locally, writes new value
  - what if the local read produced stale data?
  - what if read was OK, but concurrent updates?

- test-and-set-write(version#, new value) gives you atomic update to one record
  - master rejects the write if current version # != version#
  - so if concurrent updates, one will lost and retry 

<code>  
  while(1): <br />
    (x, ver) = read-latest(k) <br />
    if(test-and-set-write(k, ver, x+1)) <br />
      break <br />
</code>

- what if we wanted to do bank transfers?
  - from one account (record) to another
  - can t-a-s-w be used for this?
    - not in any direct way (but maybe to update a log)
  - nothing like 2pc for updating multiple records atomically
  - multi-record updates are not atomic
    - other readers can see intermediate state
    - other writers are not locked out
  - multi-record reads are not atomic
    - might read one account before xfer, other account after xfer

- could PNUTS reasonably have provided general transactions?
  - Megastore shows one way to do it
  - multi-site coordination, quite slow
  - but even Megastore has serious restrictions
    - you get transactions just within a row or related set of rows
    - e.g. within my e-mail msgs
    - but doesn't allow e.g. atomic msg delivery to multiple users

- is lack of general transactions a problem for web applications?
  - maybe not, if programmers know to expect it

<hr />

### Reading Question:
- how does PNUTS cope with Example 1 (page 2)
- Initially Alice's mother is in Alice's ACL, so mother can see photos
  1. Alice removes her mother from ACL
  2. Alice posts spring-break photos
- could her mother see update #2 but not update #1?
  - really, could mother's app server see updates in wrong order
  - esp if mother uses different site than Alice
- ACL and photo list must be in the same record
  - since PNUTS guarantees order only for updates to same record
- Alice sends updates to her record's master site in order
  - master site broadcasts via MB in order
  - MB tells other sites to apply updates in order
- What if Alice's mother:
  - reads the old ACL, that includes mother
  - reads the new photo list
  - answer: just one read of Alice's record, has both ACL and photo list
    - if record doesn't have new ACL, order says it can't have new photos either

<hr />

## Tolerating Failures

- the main players are
  1. app servers
  2. storage units
  3. record master's MB
  4. a record's master

- app server crashes midway through a set of updates
  - not a transaction, so only some of writes will happen
  - but master SU/MB either did or didn't get each write
    - so each write happens at all sites, or none

- SU crashes and quickly reboots
  - (I'm guessing here, could be wrong)
  - may have been in the middle of applying an update
  - don't want to toss SU's disk content and copy from another site!
  - so SUs must have some kind of logging and recovery
  - slave SU writes disk, then sends ACK to MB
  - if MB got no ACK, MB will send to SU again
    - so SU won't miss updates while it was down
  - but:
    - if SU is master for anything, it might disagree with MB about recent commit
    - suggests maybe PNUTS doesn't do this kind of recovery?
    - or master SU asks MB to commit before updating local disk?
      - and master SU asks MB for its own recent ops after reboot?

- SU loses disk contents, or is down for a while
  - now that site is missing some data!
  - can it serve reads from local web servers by forwarding to other sites?
    - paper doesn't say
  - need to restore disk content from SUs at other sites
    1. subscribe to MB feed, and save them for now
    2. copy content from SUs at other sites
    3. replay saved MB updates

- MB crashes after accepting update
  - logs to disks on two MB server before ACKing
  - recovery looks at log, (re)sends logged msgs
  - record master may re-send an update if MB crash before ACK
    - record version #s will allow SUs to ignore duplicate

- MB is a neat idea
  - atomic: updates all replicas, or none
    - so failure of app srvrs isn't a problem
  - reliable: keeps trying, to cope with temporarily SU/site failure
  - async: apps don't have to wait for write to complete, good for WAN
  - ordered: keeps replicas identical even w/ multiple writers

- record's master site loses network connection
  - can other sites designate a replacement RM?
    - no: original RM may still be processing updates
    - don't want *two* RMs!
  - do other sites have to wait indefinitely?
  - this is what the end of section 2.2 is about -- Dynamo envy

- how to change record's master if no failures?
  - e.g. I move from Boston to LA
  - records keep track of where the previous few writes originated
  - perhaps just update the record, via old master?
    - since ID of master site is stored in the record
  - a few subsequent updates might go to the old master
    - it will reject them, app retries and finds new master

- Do they need Paxos?
  - paper does not mention agreement
  - for tablet controllers to agree on which SU stores each tablet?
  - for MBs to agree on which is primary?

<hr />

## Performance + Evaluation

Evaluation focuses on latency and scaling

- section 5.2: time for an insert while busy
  - depends on how far away Record Master is
  - RM local: 75.6 ms
  - RM nearby: 131.5 ms
  - RM other coast: 315.5 ms

- what is 5.2 measuring? from what to what?
  - maybe web server starts insert, to RM replies w/ new version?
  - probably not time for MB to propagate to all sites
    - since then local RM wouldn't be < remote

Why 75 ms?

Is it 75 ms of network speed-of-light delay?
  no: local

Is the 75 ms mostly queuing, waiting for other client's operations?
  no: they imply 100 clients was max that didn't cause delay to rise

End of 5.2 suggests 40 ms of 75 ms in in SU
  how could it take 40 ms?
    each key/value is one file?
    creating a file takes 3 disk writes (directory, inode, content)?
  what's the other 35 ms?

- But only 33 ms (not 75) for "ordered table" (MySQL/Innodb)
  - closer to the one disk write we'd expect

5.3 / Figure 3: effect of increasing request rate
  what do we expect for graph w/ x-axis req rate, y-axis latency?
    system has some inherent capacity, e.g. total disk seeks/second
    for rates less than that, constant latency
    for rates higher than that, growing queues, divergent average latency
  blow-up should be at max capacity of h/w
    e.g. # disk arms / seek time
  we don't see that in Figure 3
    end of 5.3 says clients too slow
    at >= 75 ms/op, 300 clients -> about 4000/sec
  text says max possible rate was about 3000/second
    that's higher than 1300 from section 5.2 -- why?
    probably 5.3 has lots of reads as well as writes

<hr />

## Wrapping up (key design decisions)

1. async replication
  - fast reads, but stale
  - fast writes if near master, but not visible for a while
2. relaxed consistency / no transactions
  - assumes that applications are ok with this schema
  - 5 years before spanner, in direct contrast with Spanner
  - spanner has replication across data centers and still is performant
3. primary-backup replication: sequence all writes through master site
  - pro: keeps replicas identical,
    - enforces serial order on updates,
    - easy to reason about
  - con: no progress if master site disconnected

- Next: Dynamo, a very different design
  - async replication, but no master
  - eventual consistency
  - always allow updates
  - tree of versions if network partitions
  - readers must reconcile versions
