---
layout: post
title: Spanner
category: distributed-systems
---

# Lecture 7: Spanner

<hr />
Spanner: Google's Globally-Distributed Database<br>
Corbett et al, OSDI 2012
<hr />

## Introduction

### why this paper?
- modern, high performance, driven by real-world needs
- sophisticated use of paxos
- tackles consistency
- Lab 4 a (hugely) simplified version of Spanner

### big ideas
- shard management w/ paxos replication
- high performance despite synchronous WAN replication
- fast reads by asking only the nearest replica
- consistency despite sharding (this is the real focus)
- distributed transactions

<hr />

## Sharding
- we've seen this before in FDS
- the real problem is managing configuration changes
- Spanner does a more convincing job of this than FDS

### simplified sharding outline (lab 4):
- master, paxos-replicated
- replica groups, paxos-replicated
- paxos log in each replica group
- numbered configurations
- each group eventually sees master wants a handoff
- "start handoff Num=7" op in both groups' logs
- move the shards, maybe log the shard content (!)
  - anyway each replica needs a copy of each shard
  - and you have to deal w/ slow replicas
- can't finish handoff until dst has copies at majority
- so perhaps log "replica 3 got shard 7"
- "end handoff Num=7" op in both groups' logs

- Q: what if a Put is concurrent w/ handoff?
  - master sends to dst before handoff happens?
  - client has stale view and sends it to old dst after handoff?
  - arrives at either during handoff?
  - have to immediately fail if client attempts to put during handoff
  - the service will be entirely unavailable while handoff/resharding happens

- Q: what if a failure during handoff?
  - may have to transfer entire disks of data

Q: can *two* groups think they are serving a shard?

Q: could old group still serve shard if can't hear master?

<hr />

## Wide-area Synchronous Replication

- idea: wide-area synchronous replication
  - goal: survive single-site disasters
  - goal: replica near customers
  - goal: don't lose any updates

considered impractical until a few years ago
  paxos too expensive, so maybe primary/backup?
  if primary waits for ACK from backup
    50ms network will limit throughput and cause palpable delay
    esp if app has to do multiple reads at 50ms each
  if primary does not wait, it will reply to client before durable
  danger of split brain; can't make network reliable

- what's changed?
  - other site may be only 5 ms away -- San Francisco / Los Angeles
  - faster/cheaper WAN
  - apps written to tolerate delays
    - paxos w/replicas in SF and NY are going to have 50ish ms delays (imperceptible)
    - may make many slow read requests
    - but issue them in parallel
    - maybe time out quickly and try elsewhere, or redundant gets
  - huge # of concurrent clients lets you get high throughput despite high delay
    - run their requests in parallel
  - people appreciate paxos more and have streamlined variants
    - fewer msgs
      - page 9 of paxos paper: 1 round per op w/ leader + preprepare
      - paper's scheme a little more involved b/c they must ensure
        - there's at most one leader
  - read at any replica

<hr />

## Performance

- we're looking at these as if they were just a simple lab 3b style paxos
  - k/v store -- plus read at any replica.

- Table 3
- latency
  - why doesn't write latency go up w/ more replicas?
  - why does std dev of latency go down w/ more replicas?
  - r/o a *lot* faster since not a paxos agreement + use closest replica
- throughput
  - why does read throughput go up w/ # replicas?
  - why doesn't write throughput go up?
  - does write thruput seem to be going down?
- what can we conclude from Table 3?
  - is the system fast? slow?
- how fast do your paxoses run?
- Figure 5
- npaxos=5, all leaders in same zone
- why does killing a non-leader in each group have no effect?
- for killing all the leaders ("leader-hard")
  - why flat for a few seconds?
  - what causes it to start going up?
  - why does it take 5 to 10 seconds to recover?
  - why is slope *higher* until it rejoins?

- spanner reads from any paxos replica
  - read does *not* involve a paxos agreement
  - just reads the data directly from replica's k/v DB
  - for speed -- nearest may be in same room, others may be across country

Q: could we *write* to just one replica?

- Q: is reading from any replica correct? (asking the nearest paxos replica for info)
  - may not have seen latest puts
  - will serve up stale data if you do nothing special to account for this

<hr />

## Snapshot Reads <small>Keeping Read Consistency</small>

### Problem
- photo sharing site
  - i have photos
  - i have an ACL (access control list) saying who can see my photos

1. I write ACL on group A (bare majority), then
2. I add image on group B (bare majority)
3. mom reads ACL
  - may get old data from lagging group A replica
  - happens "after" step 2
4. mom reads image
  - may get new data from group B
  - happens "before" w1

This system is not acting like a single server!
There was not really any point at which the image was present but the ACL hadn't been updated

- this problem is caused by a combination of
  - partitioning -- replica groups operate separately
  - cutting corners for performance -- read from any replica

### Solution

- possibilities
  - A. make reads see latest data
     - e.g. full paxos for reads
     - expensive!
  - B. make reads see *consistent* data
     - data as it existed at *some* previous point in time
     - i.e. before #1, between #1 and #2, or after #2
     - this turns out to be much cheaper
     - spanner does this

- here's a super-simplification of spanner's consistency story for r/o clients
  - "snapshot" or "lock-free" reads (consistent, but not fresh)
  - assume for now that all the clocks agree
  - server (paxos leader) tags each write with the time at which it occurred
  - k/v DB stores *multiple* values for each key,
    - each with a different time
  - reading client picks a time t
    - for each read
      - ask relevant replica to do the read at time t
  - how does a replica read a key at time t?
    - return the stored value with highest time <= t
  - but wait, the replica may be behind
    - that is, there may be a write at time < t, but replica hasn't seen it
    - so replica must somehow be sure it has seen all writes <= t
    - idea: has it seen *any* operation from time > t?
      - if yes, and paxos group always agrees on ops in time order,
        - it's enough to check/wait for an op with time > t
      - that is what spanner does on reads (4.1.3)
  - what time should a reading client pick?
    - "now" may be slow -- may force replicas to wait
    - so perhaps a little in the past
    - client may miss latest updates
    - but at least it will see consistent snapshot
    - in our example, won't see new image w/o also seeing ACL update

How does that fix our ACL/image example?
1. I write ACL, group A assigns it time=10, then
2. I add image, group B assigns it time=15 (must be after 10)
3. mom picks a time, for example t=14
4. mom reads ACL t=14 from lagging group A replica
  - lagging, so it won't have seen a Paxos write from >= 14
  - so it knows to wait, or tell mom to try another replica
5. mom reads image from group B at t=14
  - image may have been written on that replica
  - but it will know to *not* return it since image's time is 15

Q: is it reasonable to assume that different computers' clocks agree? why might they not agree?
- drift, etc.

Q: what may go wrong if servers' clocks don't agree?
- having non-agreeing clocks simply breaks the correctness of this scheme

- a performance problem: reading client may pick time in the future, forcing reading replicas to wait to "catch up"

- a correctness problem:
  - again, the ACL/image example
  - group A and group B disagree about what time it is
  1. I write ACL on group A -- stamped with time=15
  2. I add image on group B -- stamped with time=10
  - now a client read at t=14 will see image but not ACL update

Q: why doesn't spanner just ensure that the clocks are all correct?
- after all, it has all those master GPS / atomic clocks
- network latency, clock drift, etc.

<hr />

## TrueTime (section 3)
- there is an actual "absolute" time t_abs
  - but server clocks are typically off by some unknown amount
  - TrueTime can bound the error
- so now() yields an interval: (earliest,latest)
- earliest and latest are ordinary scalar times
  - perhaps microseconds since Jan 1 1970
- t_abs is highly likely to be between earliest and latest

Q: how does TrueTime choose the interval?

spanner assigns each write a time
  might not be the actual absolute time
  but is chosen to ensure consistency

the danger:
- write 1 at group A, A's interval is (20,30)
  - is any time in that interval OK?
- then write 2 at group B, B's interval is (11,21)
  - is any time in that interval OK?
- if they are not careful, might get s1=25 s2=15
so what we want is:
- if W2 starts after W1 finishes, then s2 > s1
- simplified "external consistency invariant" from 4.1.2

how does spanner assign times to writes?
- (again, this is much simplified, see 4.1.2)
- a write request arrives at paxos leader
- it sets s to the "latest" from the current TrueTime interval
  - this is "Start" in 4.1.2
- then it *delays* until s < "earliest"
  - i.e. until s is in the past at all servers
  - this is "commit wait" in 4.1.2
- then it runs paxos to cause the write to happen

does this work for our example?
- W1 at group A, TrueTime says (20,30)
  - s1 = 30
  - commit wait until TrueTime says (31,41)
  - reply to client
- W2 at group B, TrueTime *must* now say >= (21,31)
  - (otherwise TrueTime is broken)
  - s2 = 31
  - commit wait until TrueTime says (32,43)
  - reply to client
- it does work for this example:
  - the client observed that W1 finished before S2 started,
  - and indeed s2 > s1
  - even though B's TrueTime clock was slow by the most it could be

steps:
1. s = now().latest
2. "commit delay" until s <= now().earliest
3. run paxos
4. reply to client

why the "Start" rule?
- i.e. why choose the time at the end of the TrueTime interval?
- want a timestamp that is greater than the timestamp chosen by any completed write
- previous writers waited only until their timestamps were barely < t_abs
- new writer must choose s greater than any completed write
- minimum s >= t_abs is the end of the interval

why the "Commit Wait" rule?
- ensures that s < t_abs
- otherwise write might complete with an s in the future
  - and would let Start rule give too low an s to a subsequent write

why commit *wait*; why not write value with chosen time?
- indirectly forces subsequent write to have high enough s
  - the system has no other way to communicate minimum acceptable next s
- waiting forces writes that some external agent is serializing
  - to have monotonically increasing timestamps
- w/o wait, our example goes back to s1=30 s2=21

this answers today's Question
- a large uncertainty requires a long commit wait
- as long as the time is always contained within the interval, there is no correctness problem
- (at 10s, though, you run into issues with leader election)

let's catch our breath
- why did we get into all this timestamp stuff?
  - our replicas were 100s or 1000s of miles apart (for locality/fault tol)
  - we wanted fast reads from a local replica (no full paxos)
  - our data was partitioned over many replica groups w/ separate clocks
  - we wanted consistency for reads:
    - if W1 then W2, reads don't see W2 but not W1
- it's complex but it makes sense as a high-performance evolution of Lab 3 / Lab 4

why is this timestamp technique interesting?
  we want to enforce order -- things that happened in some order in real time are ordered the same way by the
    distributed system -- "external consistency"
  the naive approach requires a central agent, or lots of communication
  Spanner does the synchronization implicitly via time
    time can be a form of communication
    e.g. we agree in advance to meet for dinner at 6:00pm

there's a lot of additional complexity in the paper
  transactions, two phase commit, two phase locking,
    schema change, query language, &c
  some of this we'll see more of later
  in particular, the problem of ordering events in a
    distributed system will come up a lot, soon